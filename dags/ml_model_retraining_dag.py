#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”„ ML Model Retraining DAG - Periodic Model Retraining with MLflow
================================================================

ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ DAG Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ML Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:
- Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· S3
- ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸
- ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
- Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ² MLflow Model Registry
- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (Champion)
- Ğ£Ğ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ

Ğ Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ: ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾ Ğ² 02:00 UTC
ĞĞ²Ñ‚Ğ¾Ñ€: ML Engineering Team
Ğ”Ğ°Ñ‚Ğ°: 2024
"""

import logging
import json
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable
from airflow.utils.trigger_rule import TriggerRule
from airflow.utils.task_group import TaskGroup
from airflow.providers.yandex.operators.dataproc import (
    DataprocCreateClusterOperator,
    DataprocCreatePysparkJobOperator,
    DataprocDeleteClusterOperator
)
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.providers.email.operators.email import EmailOperator
import uuid
from urllib.parse import urlparse

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯ DAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

default_args = {
    'owner': 'ml-engineer',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'email': ['ml-team@fraud-detection.local'],
    'retries': 2,
    'retry_delay': timedelta(minutes=15),
}

dag = DAG(
    'ml_model_retraining_pipeline',
    default_args=default_args,
    description='ğŸ”„ Periodic ML Model Retraining with MLflow Integration',
    schedule_interval='0 2 * * *',  # Ğ•Ğ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾ Ğ² 02:00 UTC
    catchup=False,
    tags=['ml', 'retraining', 'mlflow', 'fraud-detection', 'production'],
    max_active_runs=1,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ Ğ’Ğ¡ĞŸĞĞœĞĞ“ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def validate_retraining_config(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"""
    logger.info("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")
    
    required_vars = [
        'FOLDER_ID', 'SUBNET_ID', 'SECURITY_GROUP_ID',
        'S3_BUCKET_SCRIPTS', 'S3_ACCESS_KEY', 'S3_SECRET_KEY',
        'DATAPROC_SERVICE_ACCOUNT_ID',
        'MLFLOW_TRACKING_URI',
        'MLFLOW_MODEL_NAME'
    ]
    
    missing_vars = []
    for var in required_vars:
        try:
            value = Variable.get(var)
            if not value:
                missing_vars.append(var)
        except:
            missing_vars.append(var)
    
    if missing_vars:
        raise ValueError(f"âŒ ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ: {missing_vars}")
    
    # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸
    mlflow_uri = Variable.get('MLFLOW_TRACKING_URI')
    model_name = Variable.get('MLFLOW_MODEL_NAME')
    
    context['task_instance'].xcom_push(key='mlflow_uri', value=mlflow_uri)
    context['task_instance'].xcom_push(key='model_name', value=model_name)
    
    logger.info(f"âœ… MLflow URI: {mlflow_uri}")
    logger.info(f"âœ… Model Name: {model_name}")
    logger.info("âœ… ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ")
    
    return True

def check_data_freshness(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"""
    logger.info("ğŸ“Š ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑĞ²ĞµĞ¶ĞµÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ...")
    
    # Ğ—Ğ´ĞµÑÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² S3
    # Ğ”Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ mock-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ
    
    execution_date = context['execution_date']
    days_since_last_training = Variable.get('DAYS_SINCE_LAST_TRAINING', '7')
    min_new_records = Variable.get('MIN_NEW_RECORDS_FOR_RETRAINING', '10000')
    
    # Mock Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸
    data_freshness = {
        'last_training_date': (execution_date - timedelta(days=int(days_since_last_training))).isoformat(),
        'new_records_available': int(min_new_records) + 5000,  # Ğ˜Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        'data_quality_score': 0.95,
        'should_retrain': True,
        'data_drift_detected': False
    }
    
    context['task_instance'].xcom_push(key='data_freshness', value=data_freshness)
    
    if not data_freshness['should_retrain']:
        logger.info("â„¹ï¸ ĞŸĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ - Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ²ĞµĞ¶Ğ¸Ğµ")
        return False
    
    logger.info(f"âœ… Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ:")
    logger.info(f"  ğŸ“Š ĞĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹: {data_freshness['new_records_available']}")
    logger.info(f"  ğŸ“ˆ ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {data_freshness['data_quality_score']:.1%}")
    logger.info(f"  ğŸ”„ Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: {data_freshness['should_retrain']}")
    
    return True

def _s3_endpoint_host() -> str:
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ S3 endpoint host"""
    url = Variable.get('S3_ENDPOINT_URL', 'https://storage.yandexcloud.net')
    parsed = urlparse(url)
    return parsed.netloc if parsed.netloc else url

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“‹ ĞĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• Ğ—ĞĞ”ĞĞ§ - ĞŸĞ Ğ•Ğ”Ğ’ĞĞ Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ¯ ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
validate_config_task = PythonOperator(
    task_id='validate_retraining_configuration',
    python_callable=validate_retraining_config,
    dag=dag,
)

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
check_data_task = PythonOperator(
    task_id='check_data_freshness',
    python_callable=check_data_freshness,
    dag=dag,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ—ï¸ INFRASTRUCTURE SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with TaskGroup('training_infrastructure', dag=dag) as training_infrastructure:
    
    def create_training_cluster(**context):
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DataProc ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
        exec_dt = context['execution_date']
        unique_id = uuid.uuid4().hex[:6]
        cluster_name = f"ml-training-{exec_dt.strftime('%Y%m%d-%H%M%S')}-{unique_id}"
        
        logger.info(f"ğŸ·ï¸ Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {cluster_name}")
        
        folder_id = Variable.get('FOLDER_ID')
        subnet_id = Variable.get('SUBNET_ID')
        zone = Variable.get('ZONE', 'ru-central1-a')
        sa_id = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID')
        security_group_id = Variable.get('SECURITY_GROUP_ID')
        s3_logs_bucket = f"{Variable.get('S3_BUCKET_SCRIPTS')}/dataproc-logs/"
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        op = DataprocCreateClusterOperator(
            task_id='_inner_create_training_cluster',
            folder_id=folder_id,
            cluster_name=cluster_name,
            cluster_description='ML Model Training Cluster',
            subnet_id=subnet_id,
            s3_bucket=s3_logs_bucket,
            ssh_public_keys=['ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6U9uIFhz5CPUuFRvJGlaeS6sl5tbsyvsttkRReBUK+44uTnjkQs6hFTC0g7tBFm8OSUG0P6Cxrl3s4wIEo3a9VIwfFYFB5cqnwKaZU0mvmkgRoiYEG4b6rIuL7PX+CFzTOK6EtA0ZhKeF75XyO3qkAid0uucLcYqmGCE9PbgIH7sb8er0keNo3ZtZK/2ICWlSNua2uboTE312qCwGvB+P5qdEVkcen1oUskuIlwgD6Afb5bLTaEdxQiv01jsAVKiTv+dqDGbApBh1WN/97rtdDU4vLBfNpHCCBwmn/vDFLX6jhqnfhfV0NpR0SwXRBUu9Q4R0OttipPbdY9uXSXX/ airflow@dataproc'],
            zone=zone,
            cluster_image_version=Variable.get('DATAPROC_CLUSTER_VERSION', '2.0'),
            security_group_ids=[security_group_id] if security_group_id else [],
            service_account_id=sa_id,
            
            # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
            masternode_resource_preset=Variable.get('TRAINING_MASTER_PRESET', 's3-c4-m16'),
            masternode_disk_type='network-ssd',  # SSD Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
            masternode_disk_size=int(Variable.get('TRAINING_MASTER_DISK_SIZE', '100')),
            
            datanode_resource_preset=Variable.get('TRAINING_WORKER_PRESET', 's3-c8-m32'),
            datanode_disk_type='network-ssd',
            datanode_disk_size=int(Variable.get('TRAINING_WORKER_DISK_SIZE', '200')),
            datanode_count=int(Variable.get('TRAINING_WORKER_COUNT', '3')),
            
            # Ğ¡ĞµÑ€Ğ²Ğ¸ÑÑ‹ Ğ´Ğ»Ñ ML Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
            services=['YARN', 'SPARK', 'HDFS', 'MAPREDUCE'],
            dag=dag,
        )
        
        cluster_id = op.execute(context)
        logger.info(f"âœ… Training ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½: {cluster_id}")
        
        context['task_instance'].xcom_push(key='training_cluster_id', value=cluster_id)
        context['task_instance'].xcom_push(key='training_cluster_name', value=cluster_name)
        
        return cluster_id
    
    create_cluster = PythonOperator(
        task_id='create_training_cluster',
        python_callable=create_training_cluster,
        dag=dag,
    )
    
    # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
    wait_cluster_ready = BashOperator(
        task_id='wait_training_cluster_ready',
        bash_command='sleep {{ var.value.get("DATAPROC_GRACE_SECONDS", 300) }}',
        dag=dag,
    )
    
    create_cluster >> wait_cluster_ready

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– MODEL TRAINING PROCESS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with TaskGroup('model_training', dag=dag) as model_training_group:
    
    # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    prepare_training_data = DataprocCreatePysparkJobOperator(
        task_id='prepare_training_data',
        cluster_id="{{ ti.xcom_pull(task_ids='training_infrastructure.create_training_cluster', key='training_cluster_id') }}",
        main_python_file_uri=f"s3a://{Variable.get('S3_BUCKET_SCRIPTS')}/scripts/prepare_training_data.py",
        args=[
            '--s3-bucket', Variable.get('S3_BUCKET_SCRIPTS'),
            '--data-path', 'data/',
            '--output-path', 'processed_data/',
            '--validation-split', '0.2',
            '--test-split', '0.1',
            '--feature-engineering', 'true',
            '--data-quality-threshold', '0.9'
        ],
        properties={
            'spark.submit.deployMode': 'cluster',
            'spark.sql.adaptive.enabled': 'true',
            'spark.serializer': 'org.apache.spark.serializer.KryoSerializer',
            
            # S3 configuration
            'spark.hadoop.fs.s3a.endpoint': _s3_endpoint_host(),
            'spark.hadoop.fs.s3a.path.style.access': 'true',
            'spark.hadoop.fs.s3a.connection.ssl.enabled': 'true',
            'spark.hadoop.fs.s3a.impl': 'org.apache.hadoop.fs.s3a.S3AFileSystem',
            'spark.hadoop.fs.s3a.access.key': Variable.get('S3_ACCESS_KEY'),
            'spark.hadoop.fs.s3a.secret.key': Variable.get('S3_SECRET_KEY'),
            
            # Python environment
            'spark.pyspark.python': '/opt/conda/bin/python',
            'spark.yarn.appMasterEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
            'spark.executorEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
            
            # S3 credentials
            'spark.executorEnv.S3_ACCESS_KEY': Variable.get('S3_ACCESS_KEY'),
            'spark.executorEnv.S3_SECRET_KEY': Variable.get('S3_SECRET_KEY'),
            'spark.yarn.appMasterEnv.S3_ACCESS_KEY': Variable.get('S3_ACCESS_KEY'),
            'spark.yarn.appMasterEnv.S3_SECRET_KEY': Variable.get('S3_SECRET_KEY'),
        },
        connection_id='yandexcloud_default',
        dag=dag,
    )
    
    # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ MLflow
    train_model_with_mlflow = DataprocCreatePysparkJobOperator(
        task_id='train_model_with_mlflow',
        cluster_id="{{ ti.xcom_pull(task_ids='training_infrastructure.create_training_cluster', key='training_cluster_id') }}",
        main_python_file_uri=f"s3a://{Variable.get('S3_BUCKET_SCRIPTS')}/scripts/train_fraud_model_mlflow.py",
        args=[
            '--mlflow-uri', Variable.get('MLFLOW_TRACKING_URI'),
            '--experiment-name', Variable.get('MLFLOW_EXPERIMENT_NAME', 'fraud_detection_experiments'),
            '--model-name', Variable.get('MLFLOW_MODEL_NAME'),
            '--data-path', 'processed_data/',
            '--hyperparameter-tuning', 'true',
            '--cross-validation-folds', '5',
            '--max-evals', '50',
            '--auto-register', 'true'
        ],
        properties={
            'spark.submit.deployMode': 'cluster',
            'spark.sql.adaptive.enabled': 'true',
            'spark.serializer': 'org.apache.spark.serializer.KryoSerializer',
            
            # S3 configuration
            'spark.hadoop.fs.s3a.endpoint': _s3_endpoint_host(),
            'spark.hadoop.fs.s3a.path.style.access': 'true',
            'spark.hadoop.fs.s3a.connection.ssl.enabled': 'true',
            'spark.hadoop.fs.s3a.impl': 'org.apache.hadoop.fs.s3a.S3AFileSystem',
            'spark.hadoop.fs.s3a.access.key': Variable.get('S3_ACCESS_KEY'),
            'spark.hadoop.fs.s3a.secret.key': Variable.get('S3_SECRET_KEY'),
            
            # Python environment
            'spark.pyspark.python': '/opt/conda/bin/python',
            'spark.yarn.appMasterEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
            'spark.executorEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
            
            # MLflow environment
            'spark.yarn.appMasterEnv.MLFLOW_TRACKING_URI': Variable.get('MLFLOW_TRACKING_URI'),
            'spark.executorEnv.MLFLOW_TRACKING_URI': Variable.get('MLFLOW_TRACKING_URI'),
            'spark.yarn.appMasterEnv.MLFLOW_TRACKING_USERNAME': Variable.get('MLFLOW_TRACKING_USERNAME', ''),
            'spark.yarn.appMasterEnv.MLFLOW_TRACKING_PASSWORD': Variable.get('MLFLOW_TRACKING_PASSWORD', ''),
            'spark.executorEnv.MLFLOW_TRACKING_USERNAME': Variable.get('MLFLOW_TRACKING_USERNAME', ''),
            'spark.executorEnv.MLFLOW_TRACKING_PASSWORD': Variable.get('MLFLOW_TRACKING_PASSWORD', ''),
            
            # S3 credentials
            'spark.executorEnv.S3_ACCESS_KEY': Variable.get('S3_ACCESS_KEY'),
            'spark.executorEnv.S3_SECRET_KEY': Variable.get('S3_SECRET_KEY'),
            'spark.yarn.appMasterEnv.S3_ACCESS_KEY': Variable.get('S3_ACCESS_KEY'),
            'spark.yarn.appMasterEnv.S3_SECRET_KEY': Variable.get('S3_SECRET_KEY'),
        },
        connection_id='yandexcloud_default',
        dag=dag,
    )
    
    prepare_training_data >> train_model_with_mlflow

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š MODEL EVALUATION AND PROMOTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with TaskGroup('model_evaluation', dag=dag) as model_evaluation_group:
    
    def evaluate_new_model(**context):
        """ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Champion"""
        logger.info("ğŸ“Š ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ Champion...")
        
        try:
            # Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ´ĞµÑÑŒ Ğ±Ñ‹Ğ» Ğ±Ñ‹ MLflow API call
            # Ğ”Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ mock-Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
            
            current_champion_metrics = {
                'accuracy': 0.89,
                'precision': 0.87,
                'recall': 0.91,
                'f1_score': 0.89,
                'auc_roc': 0.94
            }
            
            new_model_metrics = {
                'accuracy': 0.91,    # Ğ›ÑƒÑ‡ÑˆĞµ
                'precision': 0.89,   # Ğ›ÑƒÑ‡ÑˆĞµ
                'recall': 0.93,      # Ğ›ÑƒÑ‡ÑˆĞµ
                'f1_score': 0.91,    # Ğ›ÑƒÑ‡ÑˆĞµ
                'auc_roc': 0.96      # Ğ›ÑƒÑ‡ÑˆĞµ
            }
            
            # ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾ÑƒÑˆĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            improvement_threshold = 0.02  # 2% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ
            
            evaluation_result = {
                'current_champion_metrics': current_champion_metrics,
                'new_model_metrics': new_model_metrics,
                'improvement_score': new_model_metrics['f1_score'] - current_champion_metrics['f1_score'],
                'should_promote': new_model_metrics['f1_score'] > current_champion_metrics['f1_score'] + improvement_threshold,
                'evaluation_timestamp': datetime.now().isoformat(),
                'model_version': f"v{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            }
            
            context['task_instance'].xcom_push(key='evaluation_result', value=evaluation_result)
            
            logger.info(f"ğŸ“Š Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:")
            logger.info(f"  ğŸ† Champion F1-Score: {current_champion_metrics['f1_score']:.3f}")
            logger.info(f"  ğŸ†• New Model F1-Score: {new_model_metrics['f1_score']:.3f}")
            logger.info(f"  ğŸ“ˆ Improvement: {evaluation_result['improvement_score']:.3f}")
            logger.info(f"  ğŸš€ Should Promote: {evaluation_result['should_promote']}")
            
            return evaluation_result
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
            return {
                'error': str(e),
                'should_promote': False,
                'evaluation_timestamp': datetime.now().isoformat()
            }
    
    evaluate_model = PythonOperator(
        task_id='evaluate_new_model',
        python_callable=evaluate_new_model,
        dag=dag,
    )
    
    def promote_model_to_champion(**context):
        """ĞŸÑ€Ğ¾Ğ¼Ğ¾ÑƒÑˆĞµĞ½ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Champion"""
        logger.info("ğŸš€ ĞŸÑ€Ğ¾Ğ¼Ğ¾ÑƒÑˆĞµĞ½ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Champion...")
        
        evaluation_result = context['ti'].xcom_pull(
            task_ids='model_evaluation.evaluate_new_model',
            key='evaluation_result'
        )
        
        if not evaluation_result or not evaluation_result.get('should_promote'):
            logger.info("â„¹ï¸ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾ÑƒÑˆĞµĞ½Ğ°")
            return False
        
        try:
            # Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ´ĞµÑÑŒ Ğ±Ñ‹Ğ» Ğ±Ñ‹ MLflow API call Ğ´Ğ»Ñ ÑĞ¼ĞµĞ½Ñ‹ Ğ°Ğ»Ğ¸Ğ°ÑĞ°
            promotion_result = {
                'model_name': Variable.get('MLFLOW_MODEL_NAME'),
                'new_champion_version': evaluation_result['model_version'],
                'previous_champion_archived': True,
                'promotion_timestamp': datetime.now().isoformat(),
                'metrics': evaluation_result['new_model_metrics']
            }
            
            context['task_instance'].xcom_push(key='promotion_result', value=promotion_result)
            
            logger.info(f"ğŸ‰ ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾ÑƒÑˆĞµĞ½Ğ° Ğ´Ğ¾ Champion:")
            logger.info(f"  ğŸ“¦ ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {promotion_result['model_name']}")
            logger.info(f"  ğŸ”¢ Ğ’ĞµÑ€ÑĞ¸Ñ: {promotion_result['new_champion_version']}")
            logger.info(f"  ğŸ“Š F1-Score: {promotion_result['metrics']['f1_score']:.3f}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¾ÑƒÑˆĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
            return False
    
    promote_model = PythonOperator(
        task_id='promote_model_to_champion',
        python_callable=promote_model_to_champion,
        dag=dag,
    )
    
    evaluate_model >> promote_model

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“§ NOTIFICATIONS AND CLEANUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_training_report(**context):
    """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸"""
    logger.info("ğŸ“‹ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸...")
    
    try:
        evaluation_result = context['ti'].xcom_pull(
            task_ids='model_evaluation.evaluate_new_model',
            key='evaluation_result'
        )
        
        promotion_result = context['ti'].xcom_pull(
            task_ids='model_evaluation.promote_model_to_champion',
            key='promotion_result'
        )
        
        training_cluster_id = context['ti'].xcom_pull(
            task_ids='training_infrastructure.create_training_cluster',
            key='training_cluster_id'
        )
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚
        final_report = {
            'retraining_date': context['execution_date'].isoformat(),
            'dag_run_id': context['run_id'],
            'cluster_id': training_cluster_id,
            'training_status': 'completed',
            'evaluation_result': evaluation_result or {},
            'promotion_result': promotion_result or {},
            'model_promoted': bool(promotion_result),
            'next_training_scheduled': (context['execution_date'] + timedelta(days=1)).isoformat()
        }
        
        context['task_instance'].xcom_push(key='final_report', value=final_report)
        
        logger.info("ğŸ“Š ĞĞ¢Ğ§Ğ•Ğ¢ Ğ ĞŸĞ•Ğ Ğ•ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ˜ ĞœĞĞ”Ğ•Ğ›Ğ˜")
        logger.info("=" * 50)
        logger.info(f"ğŸ“… Ğ”Ğ°Ñ‚Ğ°: {final_report['retraining_date']}")
        logger.info(f"ğŸ¯ Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: {final_report['training_status']}")
        logger.info(f"ğŸš€ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾ÑƒÑˆĞµĞ½Ğ°: {'âœ… Ğ”Ğ' if final_report['model_promoted'] else 'âŒ ĞĞ•Ğ¢'}")
        
        if evaluation_result:
            logger.info(f"ğŸ“ˆ Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ F1-Score: {evaluation_result.get('improvement_score', 0):.3f}")
        
        logger.info("=" * 50)
        
        return final_report
        
    except Exception as e:
        logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°: {e}")
        return {
            'error': str(e),
            'retraining_date': context['execution_date'].isoformat()
        }

# Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°
generate_report = PythonOperator(
    task_id='generate_training_report',
    python_callable=generate_training_report,
    trigger_rule=TriggerRule.ALL_DONE,
    dag=dag,
)

# Email ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ…
send_training_notification = EmailOperator(
    task_id='send_training_notification',
    to=['ml-team@fraud-detection.local', 'devops@fraud-detection.local'],
    subject='ğŸ”„ ML Model Retraining Completed - {{ ds }}',
    html_content="""
    <h2>ğŸ”„ ML Model Retraining Report</h2>
    
    <p><strong>Date:</strong> {{ ds }}</p>
    <p><strong>DAG Run ID:</strong> {{ run_id }}</p>
    
    <h3>ğŸ“Š Results:</h3>
    <ul>
        <li><strong>Training Status:</strong> {{ ti.xcom_pull(task_ids='generate_training_report', key='final_report')['training_status'] | default('unknown') }}</li>
        <li><strong>Model Promoted:</strong> {{ 'Yes âœ…' if ti.xcom_pull(task_ids='generate_training_report', key='final_report')['model_promoted'] else 'No âŒ' }}</li>
    </ul>
    
    <h3>ğŸ”— Links:</h3>
    <ul>
        <li><a href="http://mlflow-server.airflow.svc.cluster.local:5000">MLflow UI</a></li>
        <li><a href="http://grafana.monitoring.svc.cluster.local:3000">Grafana Dashboard</a></li>
        <li><a href="http://airflow-webserver.airflow.svc.cluster.local:8080">Airflow UI</a></li>
    </ul>
    
    <p>Next training scheduled for: <strong>{{ (execution_date + macros.timedelta(days=1)).strftime('%Y-%m-%d') }}</strong></p>
    """,
    trigger_rule=TriggerRule.ALL_DONE,
    dag=dag,
)

# Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
delete_training_cluster = DataprocDeleteClusterOperator(
    task_id='delete_training_cluster',
    cluster_id="{{ ti.xcom_pull(task_ids='training_infrastructure.create_training_cluster', key='training_cluster_id') }}",
    trigger_rule=TriggerRule.ALL_DONE,
    dag=dag,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”— ĞĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞĞ¡Ğ¢Ğ•Ğ™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ flow
validate_config_task >> check_data_task >> training_infrastructure

# ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
training_infrastructure >> model_training_group

# ĞÑ†ĞµĞ½ĞºĞ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾ÑƒÑˆĞµĞ½
model_training_group >> model_evaluation_group

# ĞÑ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ°
model_evaluation_group >> [generate_report, send_training_notification] >> delete_training_cluster
