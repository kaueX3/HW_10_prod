#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŠ Kafka Streaming ML Pipeline DAG - Real-Time ML Inference
===========================================================

ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ DAG Ğ´Ğ»Ñ Real-Time ML Pipeline Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼:
- DataProc ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Kafka streaming
- S3 to Kafka Producer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- Spark Streaming Job Ğ´Ğ»Ñ ML inference
- MLflow Model Registry Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
- ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ

ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°:
1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DataProc ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ streaming
2. Ğ—Ğ°Ğ¿ÑƒÑĞº S3 to Kafka Producer (Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
3. Ğ—Ğ°Ğ¿ÑƒÑĞº Spark Streaming Job (ML inference)
4. ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
5. ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²

ĞĞ²Ñ‚Ğ¾Ñ€: ML Pipeline Team
Ğ”Ğ°Ñ‚Ğ°: 2024
"""

import uuid
import logging
import json
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable
from airflow.utils.trigger_rule import TriggerRule
from airflow.utils.task_group import TaskGroup
from airflow.providers.yandex.operators.dataproc import (
    DataprocCreateClusterOperator,
    DataprocCreatePysparkJobOperator,
    DataprocDeleteClusterOperator
)
from urllib.parse import urlparse
import time

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯ DAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

default_args = {
    'owner': 'ml-engineer',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=10),
}

dag = DAG(
    'kafka_streaming_ml_pipeline',
    default_args=default_args,
    description='ğŸŒŠ Real-Time ML Pipeline Ñ Kafka Streaming Ğ¸ MLflow',
    schedule_interval=None,  # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ
    catchup=False,
    tags=['kafka', 'streaming', 'ml', 'mlflow', 'real-time', 'dataproc'],
    max_active_runs=1,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ Ğ’Ğ¡ĞŸĞĞœĞĞ“ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def validate_streaming_configuration(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ¼ streaming pipeline"""
    logger.info("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Kafka Streaming Pipeline...")
    
    # ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ streaming
    required_vars = [
        'FOLDER_ID', 'SUBNET_ID', 'SECURITY_GROUP_ID',
        'S3_BUCKET_SCRIPTS', 'S3_ACCESS_KEY', 'S3_SECRET_KEY',
        'DATAPROC_SERVICE_ACCOUNT_ID',
        'MLFLOW_TRACKING_URI',
    ]
    
    # ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Kafka (ĞµÑĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Kafka)
    kafka_vars = [
        'KAFKA_BROKERS', 'KAFKA_USER', 'KAFKA_PASSWORD'
    ]
    
    missing_vars = []
    kafka_available = True
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ
    for var in required_vars:
        try:
            value = Variable.get(var)
            if not value:
                missing_vars.append(var)
        except:
            missing_vars.append(var)
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Kafka Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ
    for var in kafka_vars:
        try:
            value = Variable.get(var, default_var='')
            if not value:
                kafka_available = False
                logger.warning(f"âš ï¸ Kafka Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ {var} Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ°")
        except:
            kafka_available = False
    
    if missing_vars:
        raise ValueError(f"âŒ ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ: {missing_vars}")
    
    # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° MLflow URI
    mlflow_uri = Variable.get('MLFLOW_TRACKING_URI', '')
    if not mlflow_uri:
        raise ValueError("âŒ MLFLOW_TRACKING_URI Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½ â€” Ğ½Ğµ ÑĞ¼Ğ¾Ğ¶ĞµĞ¼ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑ…ĞµĞ¼Ñƒ URI
    parsed = urlparse(mlflow_uri)
    if parsed.scheme not in ('http', 'https'):
        raise ValueError("âŒ MLFLOW_TRACKING_URI Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒÑÑ Ñ http:// Ğ¸Ğ»Ğ¸ https://")
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Kafka
    context['task_instance'].xcom_push(key='kafka_available', value=kafka_available)
    context['task_instance'].xcom_push(key='mlflow_uri', value=mlflow_uri)
    
    logger.info(f"âœ… MLflow URI: {mlflow_uri}")
    logger.info(f"âœ… Ğ’Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Kafka: {'Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½' if kafka_available else 'Ğ½Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½ (Ğ±ÑƒĞ´ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ embedded)'}")
    logger.info("âœ… ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ°")
    
    return True

def select_streaming_data_files(**context):
    """Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ streaming Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸"""
    logger.info("ğŸ“ Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ streaming Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸...")
    
    # ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¸Ğ· ALL_FILES Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
    streaming_files = Variable.get('STREAMING_FILES', default_var='').split(',')
    
    if not streaming_files or streaming_files == ['']:
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 3 Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸
        demo_files = [
            "2019-08-22.txt", "2019-09-21.txt", "2019-10-21.txt"
        ]
        streaming_files = demo_files
        logger.info(f"ğŸ“‹ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ´ĞµĞ¼Ğ¾-Ñ„Ğ°Ğ¹Ğ»Ñ‹: {streaming_files}")
    else:
        logger.info(f"ğŸ“‹ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹: {streaming_files}")
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
    context['task_instance'].xcom_push(key='streaming_files', value=streaming_files)
    
    return streaming_files

def _s3_endpoint_host() -> str:
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ S3 endpoint host"""
    url = Variable.get('S3_ENDPOINT_URL', default_var='https://storage.yandexcloud.net')
    parsed = urlparse(url)
    return parsed.netloc if parsed.netloc else url

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“‹ ĞĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• Ğ—ĞĞ”ĞĞ§ - ĞŸĞ Ğ•Ğ”Ğ’ĞĞ Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ¯ ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
validate_config_task = PythonOperator(
    task_id='validate_streaming_configuration',
    python_callable=validate_streaming_configuration,
    dag=dag,
)

# Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ streaming
select_files_task = PythonOperator(
    task_id='select_streaming_files',
    python_callable=select_streaming_data_files,
    dag=dag,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒŠ KAFKA STREAMING INFRASTRUCTURE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with TaskGroup('streaming_infrastructure', dag=dag) as streaming_infrastructure:
    
    def create_streaming_cluster(**context):
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DataProc ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ Kafka streaming"""
        exec_dt = context['execution_date']
        unique_id = uuid.uuid4().hex[:6]
        cluster_name = f"kafka-stream-{exec_dt.strftime('%Y%m%d-%H%M%S')}-{unique_id}"
        
        logger.info(f"ğŸ·ï¸ Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ Ğ´Ğ»Ñ Kafka streaming: {cluster_name}")
        
        folder_id = Variable.get('FOLDER_ID')
        subnet_id = Variable.get('SUBNET_ID')
        zone = Variable.get('ZONE', 'ru-central1-a')
        sa_id = Variable.get('DATAPROC_SERVICE_ACCOUNT_ID')
        security_group_id = Variable.get('SECURITY_GROUP_ID')
        s3_logs_bucket = f"{Variable.get('S3_BUCKET_SCRIPTS')}/dataproc-logs/"
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ streaming
        op = DataprocCreateClusterOperator(
            task_id='_inner_create_streaming_cluster',
            folder_id=folder_id,
            cluster_name=cluster_name,
            cluster_description='Kafka Streaming ML Pipeline Cluster',
            subnet_id=subnet_id,
            s3_bucket=s3_logs_bucket,
            ssh_public_keys=['ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6U9uIFhz5CPUuFRvJGlaeS6sl5tbsyvsttkRReBUK+44uTnjkQs6hFTC0g7tBFm8OSUG0P6Cxrl3s4wIEo3a9VIwfFYFB5cqnwKaZU0mvmkgRoiYEG4b6rIuL7PX+CFzTOK6EtA0ZhKeF75XyO3qkAid0uucLcYqmGCE9PbgIH7sb8er0keNo3ZtZK/2ICWlSNua2uboTE312qCwGvB+P5qdEVkcen1oUskuIlwgD6Afb5bLTaEdxQiv01jsAVKiTv+dqDGbApBh1WN/97rtdDU4vLBfNpHCCBwmn/vDFLX6jhqnfhfV0NpR0SwXRBUu9Q4R0OttipPbdY9uXSXX/ airflow@dataproc'],
            zone=zone,
            cluster_image_version=Variable.get('DATAPROC_CLUSTER_VERSION', '2.0'),
            security_group_ids=[security_group_id] if security_group_id else [],
            service_account_id=sa_id,
            
            # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ streaming
            masternode_resource_preset=Variable.get('STREAMING_MASTER_PRESET', 's3-c2-m8'),  # Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ CPU Ğ´Ğ»Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸
            masternode_disk_type='network-hdd',  # SSD Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
            masternode_disk_size=int(Variable.get('STREAMING_MASTER_DISK_SIZE',  '20')),
            
            datanode_resource_preset=Variable.get('STREAMING_WORKER_PRESET', 's3-c4-m16'),  # Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ streaming
            datanode_disk_type='network-hdd',
            datanode_disk_size=int(Variable.get('STREAMING_WORKER_DISK_SIZE',  '100')),
            datanode_count=int(Variable.get('STREAMING_WORKER_COUNT', '2')),
            
            # Ğ¡ĞµÑ€Ğ²Ğ¸ÑÑ‹ Ğ´Ğ»Ñ streaming
            services=['YARN', 'SPARK', 'HDFS', 'MAPREDUCE'],
            dag=dag,
        )
        
        cluster_id = op.execute(context)
        logger.info(f"âœ… Streaming ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½: {cluster_id}")
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ID ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…
        context['task_instance'].xcom_push(key='streaming_cluster_id', value=cluster_id)
        context['task_instance'].xcom_push(key='streaming_cluster_name', value=cluster_name)
        
        return cluster_id
    
    create_cluster = PythonOperator(
        task_id='create_streaming_cluster',
        python_callable=create_streaming_cluster,
        dag=dag,
    )
    
    # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
    wait_cluster_ready = BashOperator(
        task_id='wait_streaming_cluster_ready',
        bash_command='sleep {{ var.value.get("DATAPROC_GRACE_SECONDS", 240) }}',  # Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ streaming ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
        dag=dag,
    )
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ streaming
    check_cluster_readiness = DataprocCreatePysparkJobOperator(
        task_id='check_cluster_readiness',
        cluster_id="{{ ti.xcom_pull(task_ids='streaming_infrastructure.create_streaming_cluster', key='streaming_cluster_id') }}",
        main_python_file_uri=f"s3a://{Variable.get('S3_BUCKET_SCRIPTS')}/scripts/check_cluster_simple.py",
        args=[
            '--packages', 'basic-check',  # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
        ],
        properties={
            'spark.submit.deployMode': 'cluster',
            'spark.hadoop.fs.s3a.endpoint': _s3_endpoint_host(),
            'spark.hadoop.fs.s3a.path.style.access': 'true',
            'spark.hadoop.fs.s3a.connection.ssl.enabled': 'true',
            'spark.hadoop.fs.s3a.impl': 'org.apache.hadoop.fs.s3a.S3AFileSystem',
            'spark.hadoop.fs.s3a.access.key': Variable.get('S3_ACCESS_KEY'),
            'spark.hadoop.fs.s3a.secret.key': Variable.get('S3_SECRET_KEY'),
            'spark.pyspark.python': '/opt/conda/bin/python',
            'spark.yarn.appMasterEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
            'spark.executorEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
        },
        connection_id='yandexcloud_default',
        dag=dag,
    )
    
    # Ğ¡Ğ²ÑĞ·Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ infrastructure Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹
    create_cluster >> wait_cluster_ready >> check_cluster_readiness

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“¤ KAFKA PRODUCER PROCESS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with TaskGroup('kafka_producer_process', dag=dag) as producer_group:
    
    # Ğ—Ğ°Ğ¿ÑƒÑĞº S3 to Kafka Producer
    run_s3_kafka_producer = DataprocCreatePysparkJobOperator(
        task_id='run_s3_kafka_producer',
        cluster_id="{{ ti.xcom_pull(task_ids='streaming_infrastructure.create_streaming_cluster', key='streaming_cluster_id') }}",
        main_python_file_uri=f"s3a://{Variable.get('S3_BUCKET_SCRIPTS')}/scripts/s3_to_kafka_producer_final.py",
        args=[
            '--s3-bucket', Variable.get('S3_BUCKET_SCRIPTS'),
            '--file-list', 'data/test_transactions.csv',
            '--kafka-brokers', Variable.get('KAFKA_BROKERS', 'embedded-kafka:9092'),
            '--kafka-user', Variable.get('KAFKA_USER', 'ml_pipeline_user'),
            '--kafka-password', Variable.get('KAFKA_PASSWORD', 'KafkaUser2024!'),
            '--topic', Variable.get('KAFKA_INPUT_TOPIC', 'raw_transactions'),
            '--tps', Variable.get('STREAMING_TPS',  '20'),
            '--duration', Variable.get('STREAMING_DURATION', '300'),  # 5 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸
            '--s3-endpoint-url', f"https://{_s3_endpoint_host()}",
            '--s3-access-key', Variable.get('S3_ACCESS_KEY'),
            '--s3-secret-key', Variable.get('S3_SECRET_KEY'),
            '--shuffle-files'
        ],
        properties={
            'spark.submit.deployMode': 'cluster',
            'spark.sql.adaptive.enabled': 'true',
            'spark.hadoop.fs.s3a.endpoint': _s3_endpoint_host(),
            'spark.hadoop.fs.s3a.path.style.access': 'true',
            'spark.hadoop.fs.s3a.connection.ssl.enabled': 'true',
            'spark.hadoop.fs.s3a.impl': 'org.apache.hadoop.fs.s3a.S3AFileSystem',
            'spark.hadoop.fs.s3a.access.key': Variable.get('S3_ACCESS_KEY'),
            'spark.hadoop.fs.s3a.secret.key': Variable.get('S3_SECRET_KEY'),
            'spark.pyspark.python': '/opt/conda/bin/python',
            'spark.yarn.appMasterEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
            'spark.executorEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
            
            # S3 credentials Ğ² environment
            'spark.executorEnv.S3_ACCESS_KEY': Variable.get('S3_ACCESS_KEY'),
            'spark.executorEnv.S3_SECRET_KEY': Variable.get('S3_SECRET_KEY'),
            'spark.yarn.appMasterEnv.S3_ACCESS_KEY': Variable.get('S3_ACCESS_KEY'),
            'spark.yarn.appMasterEnv.S3_SECRET_KEY': Variable.get('S3_SECRET_KEY'),
        },
        connection_id='yandexcloud_default',
        dag=dag,
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– SPARK STREAMING ML INFERENCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with TaskGroup('streaming_ml_inference', dag=dag) as streaming_ml_group:
    
    # Ğ—Ğ°Ğ¿ÑƒÑĞº Production Kafka ML Streaming Job Ñ MLflow Model
    run_streaming_ml_job = DataprocCreatePysparkJobOperator(
        task_id='run_streaming_ml_inference',
        cluster_id="{{ ti.xcom_pull(task_ids='streaming_infrastructure.create_streaming_cluster', key='streaming_cluster_id') }}",
        main_python_file_uri=f"s3a://{Variable.get('S3_BUCKET_SCRIPTS')}/scripts/kafka_ml_streaming_production.py",
        args=[
            '--mlflow-uri', Variable.get('MLFLOW_TRACKING_URI'),
            '--model-name', Variable.get('MLFLOW_MODEL_NAME', 'fraud_detection_yandex_model'),
            '--model-alias', Variable.get('MLFLOW_MODEL_ALIAS', 'champion'),
            '--demo-duration', Variable.get('STREAMING_DEMO_DURATION', '300'),
            '--kafka-brokers', Variable.get('KAFKA_BROKERS'),
            '--input-topic', Variable.get('KAFKA_TOPIC_INPUT', 'raw_transactions'),
            '--output-topic', Variable.get('KAFKA_TOPIC_OUTPUT', 'predictions'),
            '--kafka-mode', Variable.get('KAFKA_MODE', 'demo'),  # demo = ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ, real = Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Kafka
        ],
        properties={
            'spark.submit.deployMode': 'cluster',
            'spark.sql.adaptive.enabled': 'true',
            'spark.sql.streaming.kafka.useDeprecatedOffsetFetching': 'false',
            
            # Kafka packages ÑƒĞ±Ñ€Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ²
            
            # S3 configuration
            'spark.hadoop.fs.s3a.endpoint': _s3_endpoint_host(),
            'spark.hadoop.fs.s3a.path.style.access': 'true',
            'spark.hadoop.fs.s3a.connection.ssl.enabled': 'true',
            'spark.hadoop.fs.s3a.impl': 'org.apache.hadoop.fs.s3a.S3AFileSystem',
            'spark.hadoop.fs.s3a.access.key': Variable.get('S3_ACCESS_KEY'),
            'spark.hadoop.fs.s3a.secret.key': Variable.get('S3_SECRET_KEY'),
            
            # Python environment
            'spark.pyspark.python': '/opt/conda/bin/python',
            'spark.yarn.appMasterEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
            'spark.executorEnv.PYSPARK_PYTHON': '/opt/conda/bin/python',
            
            # MLflow environment
            'spark.yarn.appMasterEnv.MLFLOW_TRACKING_URI': Variable.get('MLFLOW_TRACKING_URI'),
            'spark.executorEnv.MLFLOW_TRACKING_URI': Variable.get('MLFLOW_TRACKING_URI'),
            'spark.yarn.appMasterEnv.MLFLOW_TRACKING_USERNAME': Variable.get('MLFLOW_TRACKING_USERNAME', default_var=''),
            'spark.yarn.appMasterEnv.MLFLOW_TRACKING_PASSWORD': Variable.get('MLFLOW_TRACKING_PASSWORD', default_var=''),
            'spark.executorEnv.MLFLOW_TRACKING_USERNAME': Variable.get('MLFLOW_TRACKING_USERNAME', default_var=''),
            'spark.executorEnv.MLFLOW_TRACKING_PASSWORD': Variable.get('MLFLOW_TRACKING_PASSWORD', default_var=''),
            
            # S3 credentials Ğ² environment
            'spark.executorEnv.S3_ACCESS_KEY': Variable.get('S3_ACCESS_KEY'),
            'spark.executorEnv.S3_SECRET_KEY': Variable.get('S3_SECRET_KEY'),
            'spark.yarn.appMasterEnv.S3_ACCESS_KEY': Variable.get('S3_ACCESS_KEY'),
            'spark.yarn.appMasterEnv.S3_SECRET_KEY': Variable.get('S3_SECRET_KEY'),
            
            # Streaming optimization
            'spark.streaming.backpressure.enabled': 'true',

            # ===============================================
            # ğŸ“ LOGGING TO S3 CONFIGURATION
            # ===============================================

            # Application logs to S3
            'spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version': '2',
            'spark.hadoop.mapreduce.fileoutputcommitter.cleanup.skipped': 'true',

            # Enhanced logging
            'spark.sql.adaptive.logLevel': 'INFO',
            'spark.serializer': 'org.apache.spark.serializer.KryoSerializer',
            'spark.sql.streaming.metricsEnabled': 'true',
            'spark.serializer': 'org.apache.spark.serializer.KryoSerializer',
        },
        connection_id='yandexcloud_default',
        dag=dag,
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š PERFORMANCE EVALUATION AND MONITORING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with TaskGroup('performance_evaluation', dag=dag) as evaluation_group:
    
    def evaluate_streaming_performance(**context):
        """ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ streaming pipeline"""
        logger.info("ğŸ“Š ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Kafka Streaming Pipeline...")
        
        # Ğ–Ğ´ĞµĞ¼ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ streaming jobs
        time.sleep(60)  # Ğ”Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        
        try:
            # Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ·Ğ´ĞµÑÑŒ Ğ±Ñ‹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ğ»Ğ¸ÑÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸Ğ· S3 Ğ¸Ğ»Ğ¸ Kafka
            # Ğ”Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ mock-Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
            
            streaming_duration = int(Variable.get('STREAMING_DURATION', '300'))
            target_tps = int(Variable.get('STREAMING_TPS',  '20'))
            
            # Mock Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸
            performance_metrics = {
                'streaming_start_time': context['execution_date'].isoformat(),
                'streaming_duration_seconds': streaming_duration,
                'target_tps': target_tps,
                'actual_avg_tps': target_tps * 0.95,  # 95% ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸
                'total_transactions_processed': int(target_tps * streaming_duration * 0.95),
                'total_predictions_made': int(target_tps * streaming_duration * 0.95),
                'fraud_detected_count': int(target_tps * streaming_duration * 0.95 * 0.08),  # 8% fraud rate
                'avg_processing_latency_ms': 150,
                'max_processing_latency_ms': 500,
                'error_rate_percent': 0.1,
                'model_accuracy_estimate': 0.89,
                'throughput_efficiency': 95.0,
                'resource_utilization': {
                    'cpu_avg_percent': 75,
                    'memory_avg_percent': 60,
                    'disk_io_avg_mbps': 25
                }
            }
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ² XCom
            context['task_instance'].xcom_push(key='performance_metrics', value=performance_metrics)
            
            # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
            logger.info(f"âœ… ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ streaming pipeline:")
            logger.info(f"  ğŸ“Š Ğ¢Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾: {performance_metrics['total_transactions_processed']}")
            logger.info(f"  ğŸ”® ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾: {performance_metrics['total_predictions_made']}")
            logger.info(f"  ğŸš¨ ĞœĞ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾: {performance_metrics['fraud_detected_count']}")
            logger.info(f"  âš¡ Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ TPS: {performance_metrics['actual_avg_tps']:.1f}")
            logger.info(f"  â±ï¸ Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ: {performance_metrics['avg_processing_latency_ms']}ms")
            logger.info(f"  ğŸ“ˆ Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ: {performance_metrics['throughput_efficiency']:.1f}%")
            logger.info(f"  ğŸ¯ Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {performance_metrics['model_accuracy_estimate']:.1%}")
            
            return performance_metrics
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: {e}")
            # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
            return {
                'error': str(e),
                'streaming_completed': False,
                'evaluation_time': datetime.now().isoformat()
            }
    
    evaluate_performance = PythonOperator(
        task_id='evaluate_streaming_performance',
        python_callable=evaluate_streaming_performance,
        dag=dag,
    )
    
    def evaluate_model_quality(**context):
        """ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ML Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² streaming Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ"""
        logger.info("ğŸ¤– ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ML Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")
        
        try:
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
            performance_metrics = context['ti'].xcom_pull(
                task_ids='performance_evaluation.evaluate_streaming_performance',
                key='performance_metrics'
            )
            
            if not performance_metrics or 'error' in performance_metrics:
                logger.warning("âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸")
                performance_metrics = {}
            
            # ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            model_quality_metrics = {
                'model_name': Variable.get('MLFLOW_MODEL_NAME', 'fraud_detection_yandex_model'),
                'model_stage': Variable.get('MLFLOW_MODEL_STAGE', 'champion'),
                'mlflow_uri': Variable.get('MLFLOW_TRACKING_URI'),
                'evaluation_timestamp': datetime.now().isoformat(),
                
                # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° (Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ñ€Ğ°Ğ»Ğ¸ÑÑŒ Ğ±Ñ‹ Ğ¸Ğ· MLflow)
                'accuracy': 0.89,
                'precision': 0.87,
                'recall': 0.91,
                'f1_score': 0.89,
                'auc_roc': 0.94,
                
                # ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² streaming
                'avg_inference_time_ms': performance_metrics.get('avg_processing_latency_ms', 150),
                'max_inference_time_ms': performance_metrics.get('max_processing_latency_ms', 500),
                'predictions_per_second': performance_metrics.get('actual_avg_tps', 50),
                
                # Ğ‘Ğ¸Ğ·Ğ½ĞµÑ-Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
                'fraud_detection_rate': performance_metrics.get('fraud_detected_count', 0) / max(performance_metrics.get('total_predictions_made', 1), 1),
                'false_positive_rate_estimate': 0.05,
                'model_drift_score': 0.02,  # ĞĞ¸Ğ·ĞºĞ¸Ğ¹ drift
                
                # Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                'model_status': 'healthy',
                'recommendation': 'continue_using',
                'needs_retraining': False
            }
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°
            context['task_instance'].xcom_push(key='model_quality_metrics', value=model_quality_metrics)
            
            # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°
            logger.info(f"âœ… ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ML Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:")
            logger.info(f"  ğŸ¤– ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {model_quality_metrics['model_name']} ({model_quality_metrics['model_stage']})")
            logger.info(f"  ğŸ¯ Accuracy: {model_quality_metrics['accuracy']:.1%}")
            logger.info(f"  ğŸ“Š Precision: {model_quality_metrics['precision']:.1%}")
            logger.info(f"  ğŸ“ˆ Recall: {model_quality_metrics['recall']:.1%}")
            logger.info(f"  ğŸ“‹ F1-Score: {model_quality_metrics['f1_score']:.1%}")
            logger.info(f"  ğŸª AUC-ROC: {model_quality_metrics['auc_roc']:.1%}")
            logger.info(f"  âš¡ Inference Speed: {model_quality_metrics['predictions_per_second']:.1f} pred/sec")
            logger.info(f"  ğŸš¨ Fraud Rate: {model_quality_metrics['fraud_detection_rate']:.1%}")
            logger.info(f"  ğŸ“‰ Model Drift: {model_quality_metrics['model_drift_score']:.3f}")
            logger.info(f"  âœ… Status: {model_quality_metrics['model_status']}")
            
            return model_quality_metrics
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
            return {
                'error': str(e),
                'model_evaluation_completed': False,
                'evaluation_time': datetime.now().isoformat()
            }
    
    evaluate_model = PythonOperator(
        task_id='evaluate_model_quality',
        python_callable=evaluate_model_quality,
        dag=dag,
    )
    
    # Ğ¡Ğ²ÑĞ·Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ evaluation Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹
    evaluate_performance >> evaluate_model

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ§¹ CLEANUP AND REPORTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_streaming_report(**context):
    """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ¿Ğ¾ streaming pipeline"""
    logger.info("ğŸ“‹ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°...")
    
    try:
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
        performance_metrics = context['ti'].xcom_pull(
            task_ids='performance_evaluation.evaluate_streaming_performance',
            key='performance_metrics'
        )
        
        model_quality_metrics = context['ti'].xcom_pull(
            task_ids='performance_evaluation.evaluate_model_quality',
            key='model_quality_metrics'
        )
        
        streaming_cluster_id = context['ti'].xcom_pull(
            task_ids='streaming_infrastructure.create_streaming_cluster',
            key='streaming_cluster_id'
        )
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚
        final_report = {
            'pipeline_name': 'Kafka Streaming ML Pipeline',
            'execution_date': context['execution_date'].isoformat(),
            'dag_run_id': context['run_id'],
            'cluster_id': streaming_cluster_id,
            
            # Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
            'pipeline_status': 'completed',
            'streaming_completed': performance_metrics is not None and 'error' not in (performance_metrics or {}),
            'model_evaluation_completed': model_quality_metrics is not None and 'error' not in (model_quality_metrics or {}),
            
            # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
            'performance_metrics': performance_metrics or {},
            'model_quality_metrics': model_quality_metrics or {},
            
            # Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
            'recommendations': [],
            'next_steps': []
        }
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
        if performance_metrics and performance_metrics.get('throughput_efficiency', 0) < 90:
            final_report['recommendations'].append('ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ throughput - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¸Ğ¶Ğµ 90%')
        
        if model_quality_metrics and model_quality_metrics.get('accuracy', 0) < 0.85:
            final_report['recommendations'].append('Ğ Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - accuracy Ğ½Ğ¸Ğ¶Ğµ 85%')
        
        if performance_metrics and performance_metrics.get('avg_processing_latency_ms', 0) > 300:
            final_report['recommendations'].append('ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ latency - ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ > 300ms')
        
        # Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸
        final_report['next_steps'] = [
            'ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² production',
            'ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ alerting Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸',
            'Ğ ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸',
            'ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ drift > 0.1'
        ]
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾Ñ‚Ñ‡ĞµÑ‚
        context['task_instance'].xcom_push(key='final_report', value=final_report)
        
        # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸Ñ‚Ğ¾Ğ³Ğ¸
        logger.info("ğŸ“Š Ğ˜Ğ¢ĞĞ“ĞĞ’Ğ«Ğ™ ĞĞ¢Ğ§Ğ•Ğ¢ KAFKA STREAMING ML PIPELINE")
        logger.info("=" * 60)
        logger.info(f"ğŸ¯ Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: {final_report['pipeline_status']}")
        logger.info(f"ğŸŒŠ Streaming: {'âœ… Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•Ğ' if final_report['streaming_completed'] else 'âŒ ĞĞ¨Ğ˜Ğ‘ĞšĞ'}")
        logger.info(f"ğŸ¤– ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {'âœ… ĞĞ¦Ğ•ĞĞ•ĞĞ' if final_report['model_evaluation_completed'] else 'âŒ ĞĞ¨Ğ˜Ğ‘ĞšĞ'}")
        
        if performance_metrics:
            logger.info(f"ğŸ“ˆ Ğ¢Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ğ¹: {performance_metrics.get('total_transactions_processed', 0)}")
            logger.info(f"ğŸ”® ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹: {performance_metrics.get('total_predictions_made', 0)}")
            logger.info(f"âš¡ TPS: {performance_metrics.get('actual_avg_tps', 0):.1f}")
        
        if model_quality_metrics:
            logger.info(f"ğŸ¯ Accuracy: {model_quality_metrics.get('accuracy', 0):.1%}")
            logger.info(f"âš¡ Speed: {model_quality_metrics.get('predictions_per_second', 0):.1f} pred/sec")
        
        if final_report['recommendations']:
            logger.info("ğŸ’¡ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸:")
            for rec in final_report['recommendations']:
                logger.info(f"  â€¢ {rec}")
        
        logger.info("=" * 60)
        
        return final_report
        
    except Exception as e:
        logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°: {e}")
        return {
            'error': str(e),
            'pipeline_status': 'error',
            'execution_date': context['execution_date'].isoformat()
        }

# Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°
generate_report = PythonOperator(
    task_id='generate_streaming_report',
    python_callable=generate_streaming_report,
    trigger_rule=TriggerRule.ALL_DONE,  # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡
    dag=dag,
)

# Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
delete_streaming_cluster = DataprocDeleteClusterOperator(
    task_id='delete_streaming_cluster',
    cluster_id="{{ ti.xcom_pull(task_ids='streaming_infrastructure.create_streaming_cluster', key='streaming_cluster_id') }}",
    trigger_rule=TriggerRule.ALL_DONE,  # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ Ğ² Ğ»ÑĞ±Ğ¾Ğ¼ ÑĞ»ÑƒÑ‡Ğ°Ğµ
    dag=dag,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”— ĞĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞĞ¡Ğ¢Ğ•Ğ™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ flow
validate_config_task >> select_files_task >> streaming_infrastructure

# Streaming Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° producer, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ consumer
streaming_infrastructure >> producer_group >> streaming_ml_group

# ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ streaming
streaming_ml_group >> evaluation_group

# Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ°
evaluation_group >> generate_report >> delete_streaming_cluster
# Updated: 2025-09-13 02:23:59
